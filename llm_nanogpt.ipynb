{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872f707c",
   "metadata": {},
   "source": [
    "# <center> nanoGPT: Exploring Applications of Language Models </center> <a class='tocSkip'>\n",
    "MSDS 2023 Term 4 ML3 | **Maria Loraine R. Menorca**\n",
    "    \n",
    "**Learning Goals:**\n",
    "\n",
    "1. What is a language model?\n",
    "2. Describe the dataset being used. What preprocessing steps need to be done in preparation for training the model?\n",
    "3. What is self-attention? \n",
    "4. Compare and contrast the concept of attention, self-attention, and cross-attention.\n",
    "5. What is multi-head attention? \n",
    "6. What is a transformer? \n",
    "7. Describe the other components of a transformer: residual connections, layer normalization, and dropout. What purpose do each of them serve?\n",
    "8. Tune the nanoGPT model (e.g., add more epochs, adjust batch size, learning rate, embedding dimensions, dropout, other hyperparameters, etc.). Compare a set of generated samples from before and after your tuning.\n",
    "9/ As you experiment with tuning, describe your thought process. Which hyperparameters did you decide to adjust? What were your hypotheses for how it would affect the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd67b83",
   "metadata": {},
   "source": [
    "![Banner](language_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b69b04",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "Language models (LM) are statistical models that estimate the probability distributions of linguistic units, such as words or sentences [1]. There are two main categories of language models: count-based models, also known as N-gram LMs, and continuous space models like Neural LMs.\n",
    "\n",
    "Transformers and attention mechanisms [2] are architectural components that a Language model can use to capture context and relationships between words or positions in a sequence. Some of the common applications of LMs are text completion and generation, machine translation, sentiment analysis, and  text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814bbc5",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Attention mechanisms [3] in neural networks are used to selectively focus on parts of the input or output when processing sequential data. It includes 3 components - a query, set of key-value pairs, and an output which are all represented as vectors. In attention mechanisms, the weights are determined by calculating the similarity between the query and the keys. The output is obtained by getting the weighted sum of the values.\n",
    "\n",
    "\n",
    "Self-, Cross-, and Multi-Head attention are different flavors of attention used to improve an LM's performance.\n",
    "\n",
    "***Self-attention*** [2, 4] is the ability of a model to refer to the same sequence and capture relationships between different positions within that sequence. That is, all of the keys, values, and queries come from the same place.\n",
    "\n",
    "On the other hand, ***Cross-attention*** [5] allows a model to learn relationships between elements of different sequences. The query vectors are derived from one sequence, and the key-value vectors come from another sequence.\n",
    "\n",
    "Finally, ***multi-head attention*** [2] is a stack of multiple attention heads running in parallel to attend to different parts of the input sequence. Each attention head independently calculates its own weights and output which are then combined and transformed to produce a final output within the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0fe49",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers [6, 7] are a type of neural network architecture that is useful in processing sequential data. These are designed to encode the context of an input sequence into a vector representaton and decode this information subsequently into another sequence. It consists of several key components including residual connections, layer normalization, and dropout.\n",
    "\n",
    "***Residual connections*** [8] facilitate the flow of information throughout the layers of the network by using skip-connections. These are usually used to mitigate the vanishing gradient problem by providing shortcut paths for the gradients to flow directly from succeeding to preceeding layers. The vanishing gradient problem occurs when gradients become extremely small during backpropagation, preventing effective learning in deep neural networks.\n",
    "\n",
    "***Layer normalization*** [9] serves the purpose of normalizing the outputs of each layer in the network by directly computing the normalization statistics from the summed inputs to the neurons within a hidden layer. Doing so avoids the introduction of additional dependencies or correlations between training cases and allows for more flexibility in batch processing.\n",
    "\n",
    "***Dropout*** [10] is a popular regularization technique for fully-connected neural networks. It aims to prevent overfitting by randomly deactivating or \"dropping out\" a fraction of the neural units during training. By forcing the network to rely on the remaining active units, Dropout encourages the model to learn more robust and generalized patterns using the available connections and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174bbc7",
   "metadata": {},
   "source": [
    "# II. Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eeda31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.737742Z",
     "start_time": "2023-06-09T16:24:32.653081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "print(f\"Device used: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce91780",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The project uses a sample subset of the [`Tiny Shakespeare`](https://cs.stanford.edu/people/karpathy/char-rnn/) dataset consisting of around **1.1M** characters and **65** unique characters extracted from the literary works of William Shakespeare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3ec5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.744114Z",
     "start_time": "2023-06-09T16:24:35.740526Z"
    }
   },
   "outputs": [],
   "source": [
    "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0ab894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.753122Z",
     "start_time": "2023-06-09T16:24:35.746070Z"
    }
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d852bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.761379Z",
     "start_time": "2023-06-09T16:24:35.756530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78dc599a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.769200Z",
     "start_time": "2023-06-09T16:24:35.764242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09eab34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.798790Z",
     "start_time": "2023-06-09T16:24:35.771924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743437f",
   "metadata": {},
   "source": [
    "## Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e569d7b",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "As a preliminary step, a mapping between characters and integers as indices are created for each token or string within the text. An encoder of strings into lists of integers, and a decoder of lists of integers back into strings were also defined for convenient conversion between textual and numerical data.\n",
    "\n",
    "1. `stoi = { ch:i for i,ch in enumerate(chars) }` : This line creates a dictionary called stoi (string-to-index) that maps each character in the chars list to its corresponding index or integer value. The enumerate() function generates pairs of indices and characters, which are then used to create the dictionary.\n",
    "\n",
    "2. `itos = { i:ch for i,ch in enumerate(chars) }` : This line creates a dictionary called itos (index-to-string) that maps each index to its corresponding character from the chars list.\n",
    "\n",
    "3. `encode = lambda s: [stoi[c] for c in s]` : This line defines an encoder function named encode. It takes a string s as input and converts it into a list of integers by iterating over each character c in the string and retrieving its corresponding index from the stoi dictionary.\n",
    "\n",
    "4. `decode = lambda l: ''.join([itos[i] for i in l])` : This line defines a decoder function named decode. It takes a list of integers l as input and converts it back into a string by iterating over each integer i in the list and retrieving its corresponding character from the itos dictionary. The retrieved characters are then joined together using the join() function to form the decoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69d5b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.807053Z",
     "start_time": "2023-06-09T16:24:35.800780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e317956",
   "metadata": {},
   "source": [
    "### Text Encoding\n",
    "\n",
    "The text dataset was then encoded as a PyTorch tensor allowing for efficient storage, manipulation, and processing for deep learning tasks.\n",
    "\n",
    "`data = torch.tensor(encode(text), dtype=torch.long)` : This line encodes the `text` dataset, presumably a string of characters, using the previously defined `encode` function. The `encode` function converts the text into a list of integers representing the characters. The resulting list is then converted into a PyTorch tensor using `torch.tensor()`. The `dtype=torch.long` argument specifies that the tensor should have a data type of long, which is typically used for integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e528c677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.967102Z",
     "start_time": "2023-06-09T16:24:35.809005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c49ed8",
   "metadata": {},
   "source": [
    "### Splitting & Batching\n",
    "\n",
    "The encoded data was then split into training and validation sets based on a specified ratio, `n`. For this case, 90% of the entire dataset was used for training, and the rest for validation. A block size was also defined to determine the length of subsequences used for training or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd0ae16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.977553Z",
     "start_time": "2023-06-09T16:24:35.969221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Define the block size\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30e0284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:35.987673Z",
     "start_time": "2023-06-09T16:24:35.981484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd06cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:36.002374Z",
     "start_time": "2023-06-09T16:24:35.989522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f590484",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88abec9",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "\n",
    "A Bigram language model that can be used for both training using the calculated loss, and generating new sequences by sampling from a distribution was defined. \n",
    "\n",
    "1. The `forward` method takes an input sequence `idx` and optional `targets` for training. It computes the logits (unnormalized probabilities) for the next token prediction based on the input sequence. If `target`s are provided, it calculates the cross-entropy loss between the predicted logits and the actual targets.\n",
    "\n",
    "2. The `generate` method generates new tokens given an input sequence `idx` and a maximum number of tokens to generate `max_new_tokens`. It repeatedly predicts the next token by sampling from the softmax distribution over the logits and appends the sampled token to the running sequence. The generated sequence is returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709fd5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:36.012464Z",
     "start_time": "2023-06-09T16:24:36.004789Z"
    }
   },
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "318ef783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:36.040474Z",
     "start_time": "2023-06-09T16:24:36.014642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee026c66",
   "metadata": {},
   "source": [
    "For this case, `AdamW` was chosen as the optimization algorithm, which is an extension of the Adam optimizer with weight decay regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207f3579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:36.047891Z",
     "start_time": "2023-06-09T16:24:36.043586Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b2169",
   "metadata": {},
   "source": [
    "The neural network model was then trained using mini-batch gradient descent.\n",
    "\n",
    "In each step, a batch of training data (`xb` and `yb`) is sampled using the `get_batch` function. The batch size is set to 32, meaning that 32 input sequences and their corresponding targets are processed together.\n",
    "\n",
    "The model `m` is then used to compute the logits (unnormalized probabilities) and the associated loss between the predicted logits and the actual targets. This is done by calling `m(xb, yb)`, which returns the logits and the loss.\n",
    "\n",
    "Before computing the gradients, the optimizer's `zero_grad` function is called to reset the gradients of the model parameters to zero. This is necessary because PyTorch accumulates gradients by default for each parameter, so we need to clear the gradients from the previous iteration.\n",
    "\n",
    "The loss is then backpropagated through the model using `loss.backward()`, which computes the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "Finally, the optimizer's `step` function is called to update the model parameters based on the computed gradients and the chosen optimization algorithm (AdamW in this case). This step performs the gradient descent update, adjusting the model parameters to minimize the loss.\n",
    "\n",
    "After completing all the steps, the final loss value (`loss.item()`) is printed, representing the loss achieved after the last training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319d32ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.624479Z",
     "start_time": "2023-06-09T16:24:36.051588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.574859619140625\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2db0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.788697Z",
     "start_time": "2023-06-09T16:24:37.705564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lROZtgqVuoCbMfq!H.ukmspuW,OM!,OFfmMyg;.JknOhmyQGI&fgPhLyuWYdGXaaIo&UZ:o;tpOMdFG!vHQImqSq!hON\n",
      "O?Zz;?enfCAY;X&.Omq$RQXN3McnaZ\n",
      "tFGXAY'SYQXcPcnXg,Optp3?!AQwaiuglfjtJM-U3f:H:HY,:OAOECWS;mGGJsn3GJsuM.iajbt$V-GGCg .uPLghOnoPGU.SXJWD?sXLjGJYjtNA,-'N\n",
      "Y;oqZLXATajooKrH\n",
      "lXV-GCF&kHK&L,O?MqUigko;?zxsCCaRfbuzxn y3Vn:OyxWtXaG?erT'3KFGBexFZ?e:H!d:H.urm&TT iq3YD\n",
      "H PyjNEFpkEPyDuFww.!PW,RPzN\n",
      "kWDdnz&xGtJe!mtREFEzyd.BeXhydOVD3KKpkZYZE,..FvV,IRub-Yci&jU;aX;H&.GMHxJIIm Cg;tpZEjgl.Y$Oerz?qarrKl;oCCyGX;t:Ehop-ACb.zRh3ptq\n"
     ]
    }
   ],
   "source": [
    "# Sample \n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05a42",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "\n",
    "Self-attention was performed using the `Head` module by projecting the input tensor, calculating attention scores, applying a triangular mask, computing attention weights, and aggregating the values.\n",
    "\n",
    "In the forward pass, the input tensor `x` is projected to obtain key (`k`), query (`q`), and value (`v`) tensors. Attention scores are then computed by taking the dot product of query and key tensors, followed by scaling with `C` (the square root of the hidden dimension).\n",
    "\n",
    "The triangular mask is applied to the attention scores to prevent attending to future positions in the sequence.\n",
    "The attention scores are passed through a softmax function to obtain attention weights (`wei`), which are then subjected to dropout.\n",
    "\n",
    "Finally, the values (`v`) are weighted by the attention weights (`wei`) and aggregated to produce the output tensor (`out`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aeaf669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.801818Z",
     "start_time": "2023-06-09T16:24:37.790745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39f1eed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.811136Z",
     "start_time": "2023-06-09T16:24:37.804073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9824, 0.0176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0917, 0.6751, 0.2332, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1937, 0.1384, 0.4777, 0.1902, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0801, 0.2292, 0.0693, 0.1070, 0.5143, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0671, 0.4667, 0.1121, 0.0808, 0.2024, 0.0709, 0.0000, 0.0000],\n",
       "        [0.0193, 0.1579, 0.0204, 0.1551, 0.0546, 0.0450, 0.5476, 0.0000],\n",
       "        [0.0711, 0.4844, 0.2670, 0.0235, 0.0388, 0.0146, 0.0345, 0.0661]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256ad78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.816760Z",
     "start_time": "2023-06-09T16:24:37.813064Z"
    }
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdb7dcc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.823905Z",
     "start_time": "2023-06-09T16:24:37.819130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9051)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73eeee49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.908794Z",
     "start_time": "2023-06-09T16:24:37.825823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9981)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53e46f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:37.921856Z",
     "start_time": "2023-06-09T16:24:37.913358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9188)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc74faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.138584Z",
     "start_time": "2023-06-09T16:24:37.925747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ea769e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.153425Z",
     "start_time": "2023-06-09T16:24:38.142719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f4426c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.167628Z",
     "start_time": "2023-06-09T16:24:38.156517Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e213a2",
   "metadata": {},
   "source": [
    "### Multi-headed Attention\n",
    "\n",
    "Parallel self-attention computations using multiple heads was performed using the `MultiHeadAttention` module, while the `FeedForward` module applies a non-linear transformation to the input tensor. \n",
    "\n",
    "\n",
    "`MultiHeadAttention` : This module takes an input tensor `x` and performs self-attention computations using multiple instances of the `Head` module (defined in the previous subsection). The outputs of the individual heads are concatenated and passed through a linear projection layer and a dropout layer.\n",
    "\n",
    "`FeedForward` : This module represents a simple feed-forward network. It consists of two linear layers with a ReLU activation function between them. The input tensor `x` is passed through these layers to transform the features. A dropout layer is applied after the second linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64b20fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.179423Z",
     "start_time": "2023-06-09T16:24:38.170615Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7720be",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "The `Block` module encapsulates the communication and computation steps of a transformer block, where self-attention is performed followed by a feed-forward network transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4083dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.194099Z",
     "start_time": "2023-06-09T16:24:38.184082Z"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea6b96",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "\n",
    "The 1LayerNorm1d1 module performs layer normalization by normalizing the input tensor along the second dimension (batch dimension) using learned parameters (`gamma` and `beta`), resulting in a normalized output tensor with the same shape as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32371de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.217817Z",
     "start_time": "2023-06-09T16:24:38.203409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8788033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.228861Z",
     "start_time": "2023-06-09T16:24:38.220616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e41d6",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37948f4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.270319Z",
     "start_time": "2023-06-09T16:24:38.232200Z"
    }
   },
   "outputs": [],
   "source": [
    "def nanogpt(text, batch_size=16, block_size=32, max_iters=5000,\n",
    "            eval_interval=100, learning_rate=1e-3, device='cuda',\n",
    "            eval_iters=200, n_embd=64, n_head=4, n_layer=4, dropout=0.0):\n",
    "    \"\"\"\n",
    "    This function trains a simplified version of the GPT language model called Nano GPT.\n",
    "    It takes a text corpus as input and trains a language model that can generate new text\n",
    "    similar to the input text.\n",
    "\n",
    "    Parameters:\n",
    "    -------------------\n",
    "    text (str):\n",
    "        The input text corpus used for training the language model.\n",
    "    batch_size (int):\n",
    "        The batch size for training data. Default is 16.\n",
    "    block_size (int):\n",
    "        The sequence length or context size for each training sample. Default is 32.\n",
    "    max_iters (int):\n",
    "        The maximum number of training iterations. Default is 5000.\n",
    "    eval_interval (int):\n",
    "        The interval at which to evaluate the loss on train and validation sets. Default is 100.\n",
    "    learning_rate (float):\n",
    "        The learning rate for the optimizer. Default is 1e-3.\n",
    "    device (str):\n",
    "        The device to run the training on. Default is 'cuda'.\n",
    "    eval_iters (int):\n",
    "        The number of iterations to estimate the loss on train and validation sets. Default is 200.\n",
    "    n_embd (int):\n",
    "        The embedding dimension of the language model. Default is 64.\n",
    "    n_head (int):\n",
    "        The number of attention heads in the multi-head attention mechanism. Default is 4.\n",
    "    n_layer (int):\n",
    "        The number of transformer blocks in the language model. Default is 4.\n",
    "    dropout (float):\n",
    "        The dropout probability for regularization. Default is 0.0.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "    decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "    # Train and test splits\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "\n",
    "    # data loading\n",
    "    def get_batch(split):\n",
    "        # generate a small batch of data of inputs x and targets y\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    class Head(nn.Module):\n",
    "        \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "        def __init__(self, head_size):\n",
    "            super().__init__()\n",
    "            self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            B,T,C = x.shape\n",
    "            k = self.key(x)   # (B,T,C)\n",
    "            q = self.query(x) # (B,T,C)\n",
    "            # compute attention scores (\"affinities\")\n",
    "            wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "            wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "            wei = self.dropout(wei)\n",
    "            # perform the weighted aggregation of the values\n",
    "            v = self.value(x) # (B,T,C)\n",
    "            out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "            return out\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "        def __init__(self, num_heads, head_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "            self.proj = nn.Linear(n_embd, n_embd)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "            out = self.dropout(self.proj(out))\n",
    "            return out\n",
    "\n",
    "    class FeedFoward(nn.Module):\n",
    "        \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "        def __init__(self, n_embd):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_embd, 4 * n_embd),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * n_embd, n_embd),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "        def __init__(self, n_embd, n_head):\n",
    "            # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "            super().__init__()\n",
    "            head_size = n_embd // n_head\n",
    "            self.sa = MultiHeadAttention(n_head, head_size)\n",
    "            self.ffwd = FeedFoward(n_embd)\n",
    "            self.ln1 = nn.LayerNorm(n_embd)\n",
    "            self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.sa(self.ln1(x))\n",
    "            x = x + self.ffwd(self.ln2(x))\n",
    "            return x\n",
    "\n",
    "    # super simple bigram model\n",
    "    class BigramLanguageModel(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # each token directly reads off the logits for the next token from a lookup table\n",
    "            self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "            self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "            self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "            self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "            self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        def forward(self, idx, targets=None):\n",
    "            B, T = idx.shape\n",
    "\n",
    "            # idx and targets are both (B,T) tensor of integers\n",
    "            tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "            x = tok_emb + pos_emb # (B,T,C)\n",
    "            x = self.blocks(x) # (B,T,C)\n",
    "            x = self.ln_f(x) # (B,T,C)\n",
    "            logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "            if targets is None:\n",
    "                loss = None\n",
    "            else:\n",
    "                B, T, C = logits.shape\n",
    "                logits = logits.view(B*T, C)\n",
    "                targets = targets.view(B*T)\n",
    "                loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            return logits, loss\n",
    "\n",
    "        def generate(self, idx, max_new_tokens):\n",
    "            # idx is (B, T) array of indices in the current context\n",
    "            for _ in range(max_new_tokens):\n",
    "                # crop idx to the last block_size tokens\n",
    "                idx_cond = idx[:, -block_size:]\n",
    "                # get the predictions\n",
    "                logits, loss = self(idx_cond)\n",
    "                # focus only on the last time step\n",
    "                logits = logits[:, -1, :] # becomes (B, C)\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "                # sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "                # append sampled index to the running sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            return idx\n",
    "\n",
    "    model = BigramLanguageModel()\n",
    "    m = model.to(device)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # generate from the model\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f310d1b",
   "metadata": {},
   "source": [
    "# III. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "936b4e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:24:38.279018Z",
     "start_time": "2023-06-09T16:24:38.272954Z"
    }
   },
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5061586",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1823ca2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:30:36.229150Z",
     "start_time": "2023-06-09T16:24:38.281099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4194, val loss 2.4335\n",
      "step 400: train loss 2.3505, val loss 2.3569\n",
      "step 500: train loss 2.2965, val loss 2.3129\n",
      "step 600: train loss 2.2410, val loss 2.2500\n",
      "step 700: train loss 2.2047, val loss 2.2186\n",
      "step 800: train loss 2.1635, val loss 2.1868\n",
      "step 900: train loss 2.1238, val loss 2.1503\n",
      "step 1000: train loss 2.1024, val loss 2.1289\n",
      "step 1100: train loss 2.0705, val loss 2.1189\n",
      "step 1200: train loss 2.0396, val loss 2.0808\n",
      "step 1300: train loss 2.0243, val loss 2.0631\n",
      "step 1400: train loss 1.9928, val loss 2.0369\n",
      "step 1500: train loss 1.9699, val loss 2.0306\n",
      "step 1600: train loss 1.9627, val loss 2.0476\n",
      "step 1700: train loss 1.9412, val loss 2.0150\n",
      "step 1800: train loss 1.9098, val loss 1.9967\n",
      "step 1900: train loss 1.9082, val loss 1.9873\n",
      "step 2000: train loss 1.8838, val loss 1.9931\n",
      "step 2100: train loss 1.8734, val loss 1.9754\n",
      "step 2200: train loss 1.8621, val loss 1.9638\n",
      "step 2300: train loss 1.8550, val loss 1.9528\n",
      "step 2400: train loss 1.8423, val loss 1.9425\n",
      "step 2500: train loss 1.8172, val loss 1.9422\n",
      "step 2600: train loss 1.8302, val loss 1.9399\n",
      "step 2700: train loss 1.8168, val loss 1.9368\n",
      "step 2800: train loss 1.8013, val loss 1.9212\n",
      "step 2900: train loss 1.8052, val loss 1.9309\n",
      "step 3000: train loss 1.7970, val loss 1.9192\n",
      "step 3100: train loss 1.7700, val loss 1.9176\n",
      "step 3200: train loss 1.7499, val loss 1.9045\n",
      "step 3300: train loss 1.7564, val loss 1.9046\n",
      "step 3400: train loss 1.7589, val loss 1.8987\n",
      "step 3500: train loss 1.7387, val loss 1.8945\n",
      "step 3600: train loss 1.7267, val loss 1.8880\n",
      "step 3700: train loss 1.7284, val loss 1.8812\n",
      "step 3800: train loss 1.7209, val loss 1.8883\n",
      "step 3900: train loss 1.7241, val loss 1.8776\n",
      "step 4000: train loss 1.7136, val loss 1.8592\n",
      "step 4100: train loss 1.7108, val loss 1.8684\n",
      "step 4200: train loss 1.7077, val loss 1.8647\n",
      "step 4300: train loss 1.7007, val loss 1.8449\n",
      "step 4400: train loss 1.7053, val loss 1.8586\n",
      "step 4500: train loss 1.6878, val loss 1.8450\n",
      "step 4600: train loss 1.6867, val loss 1.8314\n",
      "step 4700: train loss 1.6836, val loss 1.8416\n",
      "step 4800: train loss 1.6687, val loss 1.8445\n",
      "step 4900: train loss 1.6717, val loss 1.8331\n",
      "step 4999: train loss 1.6647, val loss 1.8235\n",
      "\n",
      "And they bride with that yet King thou was to take Ourtuned?\n",
      "It us bartht he usque, to bardetle\n",
      "Hate away, my fears' comzorm he owns,\n",
      "Hof is heart milending, and if ensent,\n",
      "A latistriviov the does me now on you so, like die; litthus wonchiry:\n",
      "Auf the speak you love's nor\n",
      "To this deserving would that\n",
      "To Winsught their as to them, His The shire\n",
      "And Let were to\n",
      "To knom thrugh fir tression must wind.\n",
      "\n",
      "MUKENIUS:\n",
      "Marry, I hom.\n",
      "\n",
      "HENRY BOLINGBROY:\n",
      "\n",
      "Shose Warwick, stavoin cour and teyerry to-chan you!\n",
      "My firders, I mary thou contrantym of a cemmiev-sent,\n",
      "For swittle and whom the head not\n",
      "us body. I much friencess,--cas lord\n",
      "In as dettured themseled Peepent:\n",
      "Is Lady pass\n",
      "Let our graive troye so upon sure fear tall:\n",
      "Whith I sheall night, sir, I do,\n",
      "To may I want beakes againce uncest the leve bunlown'd thos welcont.\n",
      "\n",
      "QUEEN FIY:\n",
      "In unhild.\n",
      "\n",
      "KING HENRY Gentleman?\n",
      "By sevence of time the shopet eyest of Romytan's whom that that saw\n",
      "And liell made that O\n",
      "For fightlly that it\n",
      "In the glette;\n",
      "you wellough, I am with you,\n",
      "For I hursend; gentle up\n",
      "And hid spingditance and to you or love\n",
      "As be ressitions so me worting.\n",
      "\n",
      "GRUMIO:\n",
      "O thus favett now,\n",
      "An but branedy wouldIng my allied.\n",
      "\n",
      "PORINIUS:\n",
      "Love Pennery, blame, to libean thing breamn'd my have eath I die;\n",
      "Save thou for He wherth our hasts.\n",
      "Let, forther of just defevold;\n",
      "Mad there'd hears subs nithily.\n",
      "\n",
      "SBRAKEN:\n",
      "A dongurlate your heavy begut;\n",
      "Good vonby the town, must a belinence.\n",
      "\n",
      "Me I andnam Gom Three:\n",
      "Thy, delet! Here sove you,\n",
      "But won ullishered, smes on.\n",
      "\n",
      "HERMIORTEM:\n",
      "O, say, as If have to kavil scace:\n",
      "Out.\n",
      "\n",
      "PORANNIA:\n",
      "By, farew-rist are gentlecy prevons dead thy path'd that to both may\n",
      "And wound arwill-say I, give affitely,\n",
      "Fhirst more is Prove at take tway,\n",
      "And then-strough longed with that I races of the hamonglings?\n",
      "\n",
      "LADY BOLYCUS:\n",
      "What befair, But I hort.\n",
      "\n",
      "ANGELO:\n",
      "First: life he putilt our side thee, our knong of answeet.\n",
      "\n",
      "MENMARDIUS:\n",
      "You nseat, Joar at Vence stan, Jurion patient:\n",
      "In is shown's fortunds, such I he have wongra\n",
      "Marr\n"
     ]
    }
   ],
   "source": [
    "nanogpt(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c1a03",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae653741",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "Default parameters, increased `max_iters` from 5000 to 10000.\n",
    "\n",
    "***Assumptions:***\n",
    "\n",
    "Increasing the maximum iterations might give the model more opportunity to learn from the training data and improve its performance. Ideally, the effect of varying number of iterations should be monitored until the loss becomes stable or stops changing.\n",
    "\n",
    "As a con, doing so increases the training time and may have the risk of overfitting since the model could start to memorize the training data instead of learning general patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d694ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:30:36.237157Z",
     "start_time": "2023-06-09T16:30:36.232015Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "347ef3ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:42:07.364584Z",
     "start_time": "2023-06-09T16:30:36.239159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4194, val loss 2.4335\n",
      "step 400: train loss 2.3505, val loss 2.3569\n",
      "step 500: train loss 2.2965, val loss 2.3129\n",
      "step 600: train loss 2.2410, val loss 2.2500\n",
      "step 700: train loss 2.2047, val loss 2.2186\n",
      "step 800: train loss 2.1635, val loss 2.1868\n",
      "step 900: train loss 2.1238, val loss 2.1503\n",
      "step 1000: train loss 2.1024, val loss 2.1289\n",
      "step 1100: train loss 2.0705, val loss 2.1189\n",
      "step 1200: train loss 2.0396, val loss 2.0808\n",
      "step 1300: train loss 2.0243, val loss 2.0631\n",
      "step 1400: train loss 1.9928, val loss 2.0369\n",
      "step 1500: train loss 1.9699, val loss 2.0306\n",
      "step 1600: train loss 1.9627, val loss 2.0476\n",
      "step 1700: train loss 1.9412, val loss 2.0150\n",
      "step 1800: train loss 1.9098, val loss 1.9967\n",
      "step 1900: train loss 1.9082, val loss 1.9873\n",
      "step 2000: train loss 1.8838, val loss 1.9931\n",
      "step 2100: train loss 1.8734, val loss 1.9754\n",
      "step 2200: train loss 1.8621, val loss 1.9638\n",
      "step 2300: train loss 1.8550, val loss 1.9528\n",
      "step 2400: train loss 1.8423, val loss 1.9425\n",
      "step 2500: train loss 1.8172, val loss 1.9422\n",
      "step 2600: train loss 1.8302, val loss 1.9399\n",
      "step 2700: train loss 1.8168, val loss 1.9368\n",
      "step 2800: train loss 1.8013, val loss 1.9212\n",
      "step 2900: train loss 1.8052, val loss 1.9309\n",
      "step 3000: train loss 1.7970, val loss 1.9192\n",
      "step 3100: train loss 1.7700, val loss 1.9176\n",
      "step 3200: train loss 1.7499, val loss 1.9045\n",
      "step 3300: train loss 1.7564, val loss 1.9046\n",
      "step 3400: train loss 1.7589, val loss 1.8987\n",
      "step 3500: train loss 1.7387, val loss 1.8945\n",
      "step 3600: train loss 1.7267, val loss 1.8880\n",
      "step 3700: train loss 1.7284, val loss 1.8812\n",
      "step 3800: train loss 1.7209, val loss 1.8883\n",
      "step 3900: train loss 1.7241, val loss 1.8776\n",
      "step 4000: train loss 1.7136, val loss 1.8592\n",
      "step 4100: train loss 1.7108, val loss 1.8684\n",
      "step 4200: train loss 1.7077, val loss 1.8647\n",
      "step 4300: train loss 1.7007, val loss 1.8449\n",
      "step 4400: train loss 1.7053, val loss 1.8586\n",
      "step 4500: train loss 1.6878, val loss 1.8450\n",
      "step 4600: train loss 1.6867, val loss 1.8314\n",
      "step 4700: train loss 1.6836, val loss 1.8416\n",
      "step 4800: train loss 1.6687, val loss 1.8445\n",
      "step 4900: train loss 1.6717, val loss 1.8331\n",
      "step 5000: train loss 1.6640, val loss 1.8238\n",
      "step 5100: train loss 1.6721, val loss 1.8323\n",
      "step 5200: train loss 1.6575, val loss 1.8124\n",
      "step 5300: train loss 1.6635, val loss 1.8199\n",
      "step 5400: train loss 1.6530, val loss 1.8138\n",
      "step 5500: train loss 1.6533, val loss 1.7972\n",
      "step 5600: train loss 1.6598, val loss 1.8128\n",
      "step 5700: train loss 1.6560, val loss 1.8089\n",
      "step 5800: train loss 1.6382, val loss 1.8016\n",
      "step 5900: train loss 1.6439, val loss 1.8013\n",
      "step 6000: train loss 1.6376, val loss 1.8004\n",
      "step 6100: train loss 1.6374, val loss 1.7849\n",
      "step 6200: train loss 1.6477, val loss 1.7943\n",
      "step 6300: train loss 1.6286, val loss 1.7956\n",
      "step 6400: train loss 1.6289, val loss 1.7982\n",
      "step 6500: train loss 1.6270, val loss 1.7855\n",
      "step 6600: train loss 1.6251, val loss 1.7794\n",
      "step 6700: train loss 1.6229, val loss 1.7887\n",
      "step 6800: train loss 1.6237, val loss 1.7988\n",
      "step 6900: train loss 1.6150, val loss 1.7853\n",
      "step 7000: train loss 1.6108, val loss 1.7801\n",
      "step 7100: train loss 1.6164, val loss 1.7828\n",
      "step 7200: train loss 1.6123, val loss 1.7965\n",
      "step 7300: train loss 1.6109, val loss 1.7783\n",
      "step 7400: train loss 1.6158, val loss 1.7897\n",
      "step 7500: train loss 1.5965, val loss 1.7839\n",
      "step 7600: train loss 1.6049, val loss 1.7800\n",
      "step 7700: train loss 1.5977, val loss 1.7747\n",
      "step 7800: train loss 1.6048, val loss 1.7640\n",
      "step 7900: train loss 1.6087, val loss 1.7800\n",
      "step 8000: train loss 1.5949, val loss 1.7777\n",
      "step 8100: train loss 1.5978, val loss 1.7723\n",
      "step 8200: train loss 1.6035, val loss 1.7845\n",
      "step 8300: train loss 1.5924, val loss 1.7676\n",
      "step 8400: train loss 1.5950, val loss 1.7702\n",
      "step 8500: train loss 1.5974, val loss 1.7733\n",
      "step 8600: train loss 1.5930, val loss 1.7763\n",
      "step 8700: train loss 1.5873, val loss 1.7678\n",
      "step 8800: train loss 1.5735, val loss 1.7717\n",
      "step 8900: train loss 1.5888, val loss 1.7520\n",
      "step 9000: train loss 1.5847, val loss 1.7622\n",
      "step 9100: train loss 1.5732, val loss 1.7569\n",
      "step 9200: train loss 1.5881, val loss 1.7560\n",
      "step 9300: train loss 1.5718, val loss 1.7434\n",
      "step 9400: train loss 1.5838, val loss 1.7479\n",
      "step 9500: train loss 1.5794, val loss 1.7646\n",
      "step 9600: train loss 1.5758, val loss 1.7538\n",
      "step 9700: train loss 1.5695, val loss 1.7399\n",
      "step 9800: train loss 1.5633, val loss 1.7587\n",
      "step 9900: train loss 1.5663, val loss 1.7367\n",
      "step 9999: train loss 1.5636, val loss 1.7391\n",
      "\n",
      "When thy bride will kiss;\n",
      "And mad yet beauth enanting. Sight, hangs; and have will, to beceding\n",
      "Hate away, my fears are zosom:\n",
      "Young, to find I commil; I like\n",
      "es it ensel cin latest in overs, and in the woeld\n",
      "jess, lesing me up the rece wity: therein speak yea:\n",
      "That I can pate a lown morld, dellow?\n",
      "\n",
      "KING RICHARD III:\n",
      "Then a mod, rive a guilted to-disgre-forgeter and thrust fire;\n",
      "Ange the flow is all oft fight are grong myrancily\n",
      "Harring is a hones and Edward? I have knear tey it-pranch\n",
      "you carly, never now mary. You contry than\n",
      "That hense of young the men.\n",
      "Was might up in so no well owl. I much friend still might. A Eiarl, that when.\n",
      "\n",
      "LUCIO:\n",
      "A hearly to did as comments.\n",
      "My virtuous so upon six! 'e, let stribut Clanger, now then it. I come, our heread! beak\n",
      "these which, what k I with unlow;\n",
      "And slight and carides comed is,\n",
      "All I in him the name, dead? I may the oor time\n",
      "The rnown cemund nexilm; and is be than the rugless well be thou shame\n",
      "For a back watch time wout in for Glowed which we run,\n",
      "That you are to Qull, kiss. when the very, sir this doing,\n",
      "Thou that harg ember mercy mines and enemies.\n",
      "\n",
      "MENENIUS:\n",
      "O well fall whide,\n",
      "An bear with bence aInd my a caning and love.\n",
      "Load Perk to doth! O, therefend exceed. Congine. Warwied of grieve;\n",
      "By tholy will with the last; low! thy coldrement, beaut from once upon a call'd makes your shall dre.\n",
      "\n",
      "Provodent Slain,\n",
      "I be have been seem can news, that you love. I can not, I thinke numentment if teny, and,\n",
      "Till incoure of Roman: wholeful,\n",
      "Thine wilt kiss but be\n",
      "the kiss ever; for but of a-father my from mene! O,\n",
      "hereford, who strake of your dried?\n",
      "\n",
      "MERCUTIO:\n",
      "There happ.\n",
      "\n",
      "BENVOLIO:\n",
      "\n",
      "ANGELO:\n",
      "Well?\n",
      "\n",
      "JULIETI:\n",
      "Signiate, courseh, sin: as it's greetises.\n",
      "\n",
      "ELBUCKINGHAM:\n",
      "There wife it before a banish'd, reign, of my glices?\n",
      "\n",
      "LADY GEN ELOZA:\n",
      "Why.\n",
      "\n",
      "HENRY BOLINGBIO:\n",
      "Thou like it, i'll hear hear for a bid and wake; 'tis now.\n",
      "\n",
      "CORIOLANUS:\n",
      "Marry me, I wise troyal. Clupple, for on the Rigation.\n",
      "\n",
      "ISABELI:\n",
      "Should with me supperity.\n",
      "\n",
      "MARLAND:\n",
      "\n",
      "Marr\n"
     ]
    }
   ],
   "source": [
    "nanogpt(text, batch_size, block_size, max_iters,\n",
    "        eval_interval, learning_rate, device,\n",
    "        eval_iters, n_embd, n_head, n_layer, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad47ad",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "Default parameters, increased `block_size` from 32 to 64.\n",
    "\n",
    "***Assumption:***\n",
    "\n",
    "Increasing the block size can help the model capture longer-term dependencies since it can capture longer sequences of text. This can be useful in this case since generating coherent paragraphs or storyline is important.\n",
    "\n",
    "Making this change may also increase memory and computational requirements since the model needs to process and store larger sequences. Moreover, introducing larger amount of information may overwhem the model and make it harder to effectively capture relevant patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eca7f87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:42:07.372800Z",
     "start_time": "2023-06-09T16:42:07.367638Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "126e2350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:47:52.258403Z",
     "start_time": "2023-06-09T16:42:07.374571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211777 M parameters\n",
      "step 0: train loss 4.3391, val loss 4.3483\n",
      "step 100: train loss 2.6521, val loss 2.6653\n",
      "step 200: train loss 2.5204, val loss 2.5179\n",
      "step 300: train loss 2.4497, val loss 2.4702\n",
      "step 400: train loss 2.3791, val loss 2.3923\n",
      "step 500: train loss 2.3265, val loss 2.3391\n",
      "step 600: train loss 2.2855, val loss 2.2954\n",
      "step 700: train loss 2.2419, val loss 2.2676\n",
      "step 800: train loss 2.2122, val loss 2.2333\n",
      "step 900: train loss 2.1666, val loss 2.1923\n",
      "step 1000: train loss 2.1302, val loss 2.1585\n",
      "step 1100: train loss 2.0901, val loss 2.1322\n",
      "step 1200: train loss 2.0550, val loss 2.1145\n",
      "step 1300: train loss 2.0388, val loss 2.0796\n",
      "step 1400: train loss 2.0033, val loss 2.0634\n",
      "step 1500: train loss 1.9718, val loss 2.0452\n",
      "step 1600: train loss 1.9616, val loss 2.0292\n",
      "step 1700: train loss 1.9292, val loss 2.0132\n",
      "step 1800: train loss 1.9059, val loss 1.9840\n",
      "step 1900: train loss 1.8909, val loss 1.9804\n",
      "step 2000: train loss 1.8743, val loss 1.9641\n",
      "step 2100: train loss 1.8555, val loss 1.9586\n",
      "step 2200: train loss 1.8393, val loss 1.9529\n",
      "step 2300: train loss 1.8172, val loss 1.9458\n",
      "step 2400: train loss 1.8131, val loss 1.9348\n",
      "step 2500: train loss 1.7920, val loss 1.9171\n",
      "step 2600: train loss 1.7746, val loss 1.9194\n",
      "step 2700: train loss 1.7654, val loss 1.9133\n",
      "step 2800: train loss 1.7527, val loss 1.8800\n",
      "step 2900: train loss 1.7465, val loss 1.8877\n",
      "step 3000: train loss 1.7319, val loss 1.8843\n",
      "step 3100: train loss 1.7186, val loss 1.8666\n",
      "step 3200: train loss 1.7122, val loss 1.8672\n",
      "step 3300: train loss 1.7116, val loss 1.8609\n",
      "step 3400: train loss 1.6975, val loss 1.8613\n",
      "step 3500: train loss 1.6832, val loss 1.8394\n",
      "step 3600: train loss 1.6768, val loss 1.8456\n",
      "step 3700: train loss 1.6769, val loss 1.8322\n",
      "step 3800: train loss 1.6645, val loss 1.8327\n",
      "step 3900: train loss 1.6530, val loss 1.8126\n",
      "step 4000: train loss 1.6553, val loss 1.8229\n",
      "step 4100: train loss 1.6551, val loss 1.8153\n",
      "step 4200: train loss 1.6404, val loss 1.8054\n",
      "step 4300: train loss 1.6282, val loss 1.7951\n",
      "step 4400: train loss 1.6320, val loss 1.7905\n",
      "step 4500: train loss 1.6377, val loss 1.8066\n",
      "step 4600: train loss 1.6248, val loss 1.7927\n",
      "step 4700: train loss 1.6179, val loss 1.8066\n",
      "step 4800: train loss 1.6134, val loss 1.7877\n",
      "step 4900: train loss 1.6102, val loss 1.7753\n",
      "step 4999: train loss 1.6021, val loss 1.7830\n",
      "\n",
      "\n",
      "YORK:\n",
      "Ricking will that year madise, buble weranty them me?\n",
      "That such happe us comment? we that anes will my feans,\n",
      "You orm heavens, toful the covert;\n",
      "And butes if ensent, wilatise in overs,\n",
      "He wife now on than twels: now, all lise, cours:\n",
      "To carraiss hew yet lorn'd nor\n",
      "To tell death:\n",
      "I lood mother\n",
      "To Wild to do piing o' man doubt, and shore\n",
      "And of my heart\n",
      "To kindnest firs so;\n",
      "And he must wre male ofte,\n",
      "Mades of my offer froul\n",
      "Have you arm adand the Edwarms:\n",
      "if courtear tey it? the hand for his need.\n",
      "Ponce, you see, what you sorrow. When-wen;\n",
      "There with ready shall bling win the cours.\n",
      "\n",
      "Thund friom that that lord\n",
      "I can deature all.\n",
      "\n",
      "LUCIO:\n",
      "Abeat over: if pars\n",
      "Lear's courever boin so upon sixe feRn,\n",
      "As nother's, shech nows good duch deep,\n",
      "You arrach! bear that from uncest these voilsel\n",
      "And, dooson lord? let your seed\n",
      "For his I sway, poir'd prond, this man to doou time\n",
      "To ornised eyself, will there what than the cuses.\n",
      "\n",
      "Blool, man I sath;\n",
      "For for drewatch time to tee, for Glood,\n",
      "This ears than brunge. along, Quw now;\n",
      "And dend conver, her notisand,\n",
      "And toach horse galten, mere things sweet worting.\n",
      "\n",
      "GREF ARDOTEL:\n",
      "How would, cheal frieng! all be wI have a caning comman,\n",
      "And any dekeep doth onatce insencesing brough\n",
      "The shall eart I dispt; so thou for Hown\n",
      "than Turn lands! try colite end,\n",
      "And defever throught?\n",
      "\n",
      "Sell:\n",
      "That, sin this caper's lord, dong's the not.\n",
      "\n",
      "Brase:\n",
      "I prever nigness, they gate the bance horse;\n",
      "And the name ome wife teny, delelige,\n",
      "And yet of Romear whole, were in me is justs;\n",
      "He their arm, ever; for but of a--\n",
      "\n",
      "Secer:\n",
      "On!\n",
      "Yet, to proud seed\n",
      "Than shorrove my aclurber? shame, ary patene;\n",
      "On is\n",
      "'tward, come rour daughting arms, give ather\n",
      "My mah, sinstand o' the hath seem; command,\n",
      "Well, this flouth sweet all Eccry,\n",
      "Who! with art, lickoly, talk weak the sworm\n",
      "fair alos of trust. Nor hone:\n",
      "And life he punish opers to thee,\n",
      "And know--with shall for most,\n",
      "They arrose trongan ature come, and is of parfort:\n",
      "In forsh whom foot thee succe offer her degry\n",
      "'Twe\n"
     ]
    }
   ],
   "source": [
    "nanogpt(text, batch_size, block_size, max_iters,\n",
    "        eval_interval, learning_rate, device,\n",
    "        eval_iters, n_embd, n_head, n_layer, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604ee92",
   "metadata": {},
   "source": [
    "### Case 3\n",
    "\n",
    "Default parameters, increased `n_head` from 4 to 8.\n",
    "\n",
    "***Assumption:***\n",
    "\n",
    "Increasing the block size can help the model capture longer-term dependencies since it can capture longer sequences of text. The model would also become more \"expressive\" and flexible since it's able to learn more diverse representations. This can be useful in this case since generating coherent paragraphs or storyline is important.\n",
    "\n",
    "As a disadvantage, training would require more memory and computational resources. There is also a potential to overfit when the model learns patterns that are too specific to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "928383f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:47:52.266635Z",
     "start_time": "2023-06-09T16:47:52.261189Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb423bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T16:56:25.007976Z",
     "start_time": "2023-06-09T16:47:52.268822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4322, val loss 4.4217\n",
      "step 100: train loss 2.6608, val loss 2.6740\n",
      "step 200: train loss 2.5182, val loss 2.5130\n",
      "step 300: train loss 2.4444, val loss 2.4574\n",
      "step 400: train loss 2.3802, val loss 2.3877\n",
      "step 500: train loss 2.3223, val loss 2.3392\n",
      "step 600: train loss 2.2742, val loss 2.2834\n",
      "step 700: train loss 2.2369, val loss 2.2463\n",
      "step 800: train loss 2.1988, val loss 2.2203\n",
      "step 900: train loss 2.1566, val loss 2.1776\n",
      "step 1000: train loss 2.1297, val loss 2.1519\n",
      "step 1100: train loss 2.1014, val loss 2.1424\n",
      "step 1200: train loss 2.0642, val loss 2.1038\n",
      "step 1300: train loss 2.0508, val loss 2.0854\n",
      "step 1400: train loss 2.0240, val loss 2.0629\n",
      "step 1500: train loss 1.9996, val loss 2.0578\n",
      "step 1600: train loss 1.9832, val loss 2.0673\n",
      "step 1700: train loss 1.9708, val loss 2.0401\n",
      "step 1800: train loss 1.9382, val loss 2.0297\n",
      "step 1900: train loss 1.9300, val loss 2.0038\n",
      "step 2000: train loss 1.9136, val loss 2.0203\n",
      "step 2100: train loss 1.8964, val loss 2.0033\n",
      "step 2200: train loss 1.8882, val loss 1.9801\n",
      "step 2300: train loss 1.8793, val loss 1.9776\n",
      "step 2400: train loss 1.8646, val loss 1.9588\n",
      "step 2500: train loss 1.8379, val loss 1.9595\n",
      "step 2600: train loss 1.8520, val loss 1.9629\n",
      "step 2700: train loss 1.8383, val loss 1.9517\n",
      "step 2800: train loss 1.8326, val loss 1.9518\n",
      "step 2900: train loss 1.8259, val loss 1.9544\n",
      "step 3000: train loss 1.8180, val loss 1.9319\n",
      "step 3100: train loss 1.7948, val loss 1.9413\n",
      "step 3200: train loss 1.7762, val loss 1.9178\n",
      "step 3300: train loss 1.7792, val loss 1.9201\n",
      "step 3400: train loss 1.7749, val loss 1.9108\n",
      "step 3500: train loss 1.7624, val loss 1.9025\n",
      "step 3600: train loss 1.7500, val loss 1.9084\n",
      "step 3700: train loss 1.7526, val loss 1.9043\n",
      "step 3800: train loss 1.7431, val loss 1.8994\n",
      "step 3900: train loss 1.7450, val loss 1.8865\n",
      "step 4000: train loss 1.7259, val loss 1.8749\n",
      "step 4100: train loss 1.7282, val loss 1.8826\n",
      "step 4200: train loss 1.7196, val loss 1.8778\n",
      "step 4300: train loss 1.7251, val loss 1.8670\n",
      "step 4400: train loss 1.7271, val loss 1.8778\n",
      "step 4500: train loss 1.7163, val loss 1.8683\n",
      "step 4600: train loss 1.7060, val loss 1.8526\n",
      "step 4700: train loss 1.6995, val loss 1.8585\n",
      "step 4800: train loss 1.6929, val loss 1.8530\n",
      "step 4900: train loss 1.6879, val loss 1.8528\n",
      "step 4999: train loss 1.6844, val loss 1.8442\n",
      "\n",
      "\n",
      "All before\n",
      "Thow and is someth\n",
      "backen bube to take One my dalina\n",
      "My art that usquet to barderlancate away, my fackstary zorman\n",
      "Your prooffice you have now\n",
      "Whigef it entengmining is the overs, and\n",
      "Will may is was twelll not, and thus, come by prave aiss hiw youngs\n",
      "Has norfoldess togent:\n",
      "Gllood mettake only son her evily, what\n",
      "For His hangs in his somelien; thus pray nonter be son; if his shall no fled\n",
      "And, and are grone my fright\n",
      "Hastingdion\n",
      "Should the Edwarn his best asare:\n",
      "your his change care, time you bembry.\n",
      "You contrantym son, and sevien these were throand?\n",
      "\n",
      "HORTENS:\n",
      "God't welcome,.\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "Which might. Andiash, that while sea\n",
      "Indeed men venty mouth!\n",
      "\n",
      "KING HENMIXemen:\n",
      "To sain deaser a gRartal not.\n",
      "\n",
      "GLOUCENIUS:\n",
      "He good dudeed,, moxthy I wash beakes againce\n",
      "And was kis with umpon my dost welconge.\n",
      "\n",
      "KING FirstIO:\n",
      "Now desile, thought in the eatey\n",
      "Tell him is is onted not the summp will;\n",
      "Good what that the wrate Edwleest man carriy;\n",
      "For to acchward, time wout it forc;\n",
      "you gonesser, I ame it weep, asir, I wonne;\n",
      "And dead conver, her not take all not weep shigghts not bewent\n",
      "Busien to wortingin it-sone, great of word,\n",
      "Do call braven all beenIn am talched.\n",
      "\n",
      "PERKEN:\n",
      "Or you begneliment men, therefe, Reide,\n",
      "Sween it what she would in that stioly's her wherems unle;\n",
      "As there's life not just deed of through he crown,\n",
      "Nor be not in stander, brived that the not. Frams begath\n",
      "with vonburty I on not the bae; I now;\n",
      "I they nong to may mine, seem thereir,\n",
      "And yet of Romarrangele, we\n",
      "Thing griom.\n",
      "\n",
      "HORTHARD:-make sages some the dingsave, scace:\n",
      "Out.\n",
      "\n",
      "POBALO:\n",
      "Nurbe, Buring said\n",
      "My vildely preven him kLary.\n",
      "\n",
      "\n",
      "NOMUS:\n",
      "Givil.\n",
      "Fromses a rishnumer this arms, give all, comes his,\n",
      "And as morne, this sender as her of love,\n",
      "Go flanters father heaven, tho! with art, liveoly?\n",
      "\n",
      "QUEEN MIFORD:\n",
      "Why.\n",
      "\n",
      "HENRDBY:\n",
      "I hortwer, I like in theling he puon toog.\n",
      "\n",
      "HERSSS:\n",
      "And;\n",
      "She not sands your your my him\n",
      "your not trongal daughter fath, Jirest parforgen,\n",
      "Not shown's fight his supper her the woe!\n",
      "\n",
      "\n",
      "MUPE\n"
     ]
    }
   ],
   "source": [
    "nanogpt(text, batch_size, block_size, max_iters,\n",
    "        eval_interval, learning_rate, device,\n",
    "        eval_iters, n_embd, n_head, n_layer, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f0f57",
   "metadata": {},
   "source": [
    "# IV. Conclusion\n",
    "\n",
    "<span style=\"font-size: 14px\">\n",
    "    <center><b>Comparison of model performance using different parameters</b></center>\n",
    "</span>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Model</th>\n",
    "            <th>Run time (mins., s)</th>\n",
    "            <th>Train Loss</th>\n",
    "            <th>Validation Loss</th>\n",
    "            <th>Val. Loss Diff vs. Baseline (%)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Baseline</th>\n",
    "            <td>5m, 58s</td>\n",
    "            <td>1.6647</td>\n",
    "            <td>1.8235</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color: rgb(255, 255, 204);\">\n",
    "            <th>Case 1</th>\n",
    "            <th>11m, 31s</th>\n",
    "            <th>1.5636</th>\n",
    "            <th>1.7391</th>\n",
    "            <td>(-) 4.63%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Case 2</th>\n",
    "            <td>5m, 45s</td>\n",
    "            <td>1.6021</td>\n",
    "            <td>1.7830</td>\n",
    "            <td>(-) 2.22%</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Case 3</th>\n",
    "            <td>8m, 33s</td>\n",
    "            <td>1.6844</td>\n",
    "            <td>1.8442</td>\n",
    "            <td>(-) 1.14%</td>\n",
    "        </tr>      \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffaf3b",
   "metadata": {},
   "source": [
    "As shown in the table above, ***Case 1*** or increasing the maximum number if iterations generated the most significant decrease in validation loss vs. the baseline model among the scenarios explored. However, it's important to note that this improvement comes at the expense of a longer run time. The increased number of iterations led to a doubling of the training time, taking ~12 mins. compared to the baseline's ~6 mins.\n",
    "\n",
    "This trade-off between improved performance and increased computational time is a common consideration in training machine learning models. It is crucial to assess whether the performance gains obtained by increasing the number of iterations justify the additional time investment. The decision to increase the maximum number of iterations should be made based on the specific requirements of the task, the available computational resources, and the desired balance between model performance and efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "037bd9cb",
   "metadata": {},
   "source": [
    "***Baseline***\n",
    "\n",
    "And they bride with that yet King thou was to take Ourtuned?\n",
    "It us bartht he usque, to bardetle\n",
    "Hate away, my fears' comzorm he owns,\n",
    "Hof is heart milending, and if ensent,\n",
    "A latistriviov the does me now on you so, like die; litthus wonchiry:\n",
    "Auf the speak you love's nor\n",
    "To this deserving would that\n",
    "To Winsught their as to them, His The shire\n",
    "And Let were to\n",
    "To knom thrugh fir tression must wind.\n",
    "\n",
    "\n",
    "***Case 1***\n",
    "\n",
    "When thy bride will kiss;\n",
    "And mad yet beauth enanting. Sight, hangs; and have will, to beceding\n",
    "Hate away, my fears are zosom:\n",
    "Young, to find I commil; I like\n",
    "es it ensel cin latest in overs, and in the woeld\n",
    "jess, lesing me up the rece wity: therein speak yea:\n",
    "That I can pate a lown morld, dellow?\n",
    "\n",
    "\n",
    "***Case 2***\n",
    "\n",
    "YORK:\n",
    "Ricking will that year madise, buble weranty them me?\n",
    "That such happe us comment? we that anes will my feans,\n",
    "You orm heavens, toful the covert;\n",
    "And butes if ensent, wilatise in overs,\n",
    "He wife now on than twels: now, all lise, cours:\n",
    "To carraiss hew yet lorn'd nor\n",
    "To tell death:\n",
    "I lood mother\n",
    "To Wild to do piing o' man doubt, and shore\n",
    "And of my heart\n",
    "To kindnest firs so;\n",
    "And he must wre male ofte,\n",
    "Mades of my offer froul\n",
    "Have you arm adand the Edwarms:\n",
    "if courtear tey it? the hand for his need.\n",
    "Ponce, you see, what you sorrow. When-wen;\n",
    "There with ready shall bling win the cours.\n",
    "\n",
    "***Case 3***\n",
    "\n",
    "if courtear tey it? the hand for his need.\n",
    "Ponce, you see, what you sorrow. When-wen;\n",
    "There with ready shall bling win the cours.\n",
    "\t\n",
    "\n",
    "\n",
    "All before\n",
    "Thow and is someth\n",
    "backen bube to take One my dalina\n",
    "My art that usquet to barderlancate away, my fackstary zorman\n",
    "Your prooffice you have now\n",
    "Whigef it entengmining is the overs, and\n",
    "Will may is was twelll not, and thus, come by prave aiss hiw youngs\n",
    "Has norfoldess togent:\n",
    "Gllood mettake only son her evily, what\n",
    "For His hangs in his somelien; thus pray nonter be son; if his shall no fled\n",
    "And, and are grone my fright\n",
    "Hastingdion\n",
    "Should the Edwarn his best asare:\n",
    "your his change care, time you bembry.\n",
    "You contrantym son, and sevien these were throand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabc9ef",
   "metadata": {},
   "source": [
    "For qualitative comparison, the first paragraphs generated from each case were compared with that of the baseline. As shown in the snippets above, ***Case 1*** stood out as it produced the most coherent and sensible text among the three cases explored. The generated texts using Case 1 also appeared to be similar to those generated by the baseline model, maintaining the overall coherence and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b4f8b",
   "metadata": {},
   "source": [
    "# V. Generative AI Documentation\n",
    "\n",
    "ChatGPT was used in this work for the following:\n",
    "\n",
    "1. Proof read the explanations provided for the concepts.\n",
    "\n",
    "2. Simplified explanations of complex topics (e.g., Transformers; Attention)\n",
    "\n",
    "3. Generate the documentation for certain code blocks.\n",
    "\n",
    "4. Generate explanations on how certain code blocks work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293e226",
   "metadata": {},
   "source": [
    "# VI. References\n",
    "\n",
    "[1] Synced. (2018, June 7). Language Model: A Survey of the State-of-the-Art Technology. Medium. https://medium.com/syncedreview/language-model-a-survey-of-the-state-of-the-art-technology-64d1a2e5a466\n",
    "\n",
    "[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is All you Need. In arXiv (Cornell University) (Vol. 30, pp. 5998–6008). Cornell University. https://arxiv.org/pdf/1706.03762v5\n",
    "\n",
    "[3] Luong, M. (2015, August 17). Effective Approaches to Attention-based Neural Machine Translation. arXiv.org. https://arxiv.org/abs/1508.04025\n",
    "\n",
    "[4] Wydmanski, W. (2022, December 30). Self attention vs attention in transformers | MLearning.ai. Medium. https://medium.com/mlearning-ai/whats-the-difference-between-self-attention-and-attention-in-transformer-architecture-3780404382f3\n",
    "\n",
    "[5] Kalra, G. (2022, July 18). Attention Networks: A simple way to understand Cross-Attention. Medium. https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-cross-attention-3b396266d82e\n",
    "\n",
    "[6] Alammar, J. (n.d.). The Illustrated Transformer. https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "[7] Alammar, J. (n.d.-b). Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention). https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "[8] Papers with Code - Residual Connection Explained. (n.d.). https://paperswithcode.com/method/residual-connection\n",
    "\n",
    "[9] Papers with Code - Layer Normalization Explained. (n.d.). https://paperswithcode.com/method/layer-normalization\n",
    "\n",
    "[10] Luo, J. (2021). Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation. https://www.semanticscholar.org/paper/Dropout-Regularization-for-Self-Supervised-Learning-Luo-Wang/d918c11715bf8e24a81b4988916e8478c970deee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187.557px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
